%!TEX root = ../Diploma.tex


\begin{section}{Проведенные эксперименты}

\begin{subsection}{Подбор параметров по сетке}
\begin{subsubsection}{kNN}
  Для алгоритма kNN оптимальной оказалась модель с k = 13 с применением весов, зависящих от расстояния до соседа (см. п. \ref{alg:knn})

  \begin{table}[H]
  \centering
  {\begin{tabular}{|l|l|c|c|}
  \hline
  \textbf{Number of neighbors \textit{k}} & \textbf{Weights} & \textbf{Accuracy} & \textbf{Std} \\
  \hline
  k = 1 & uniform  & 0.8549 & 0.00398 \\
  \hline
  k = 1 & distance  & 0.8549 & 0.00398 \\
  \hline
  k = 3 & uniform  & 0.87634 & 0.00353 \\
  \hline
  k = 3 &  distance & 0.87596  & 0.00346 \\
  \hline
  k = 5 & uniform  & 0.88281 & 0.00304 \\
  \hline
  k = 5 & distance  & 0.88302 & 0.00295 \\
  \hline
  k = 7 & uniform  & 0.88481 & 0.00367 \\
  \hline
  k = 7 &  distance & 0.88525  & 0.00360 \\
  \hline
  k = 9 & uniform  & 0.88546 & 0.00424 \\
  \hline
  k = 9 & distance  & 0.88635 & 0.00401 \\
  \hline
  k = 11 & uniform  & 0.88560 & 0.00432 \\
  \hline
  k = 11 &  distance & 0.88737  & 0.00448 \\
  \hline
  k = 13 & uniform  & 0.88618 &  0.00394 \\
  \hline
  \textbf{k = 13} & \textbf{distance}  & \textbf{0.88844} & \textbf{0.00369} \\
  \hline
  k = 15 & uniform  & 0.88612 & 0.00359 \\
  \hline
  k = 15 &  distance & 0.88802  & 0.00272 \\
  \hline
  k = 17 & uniform  & 0.88551 & 0.00345 \\
  \hline
  k = 17 & distance  & 0.88797 & 0.00300 \\
  \hline
  k = 19 & uniform  & 0.88481 & 0.00315 \\
  \hline
  k = 19 &  distance & 0.88802  & 0.00361 \\
  \hline
  \end{tabular}}

  \caption{Подбор по сетке kNN}
  \label{grid:knn}
  \end{table}



\end{subsubsection}


\begin{subsubsection}{Наивный байесовский классификатор}
Единственным параметром, по которому проходил поиск по сетке, это параметр fit\_prior, который принимает булевое значение. Суть этого параметра в использовании предыдущих вероятностей класса или не использовании.
Наивный байесовский классификатор не показал высоких результатов:

\begin{table}[H]
\centering
{\begin{tabular}{|l|c|c|}
\hline
\textbf{Fit\_Prior} & \textbf{Accuracy} & \textbf{Std} \\
\hline
True & 0.75619  & 0.00392 \\
\hline
\textbf{False} & \textbf{0.75718}  & \textbf{0.8549} \\
\hline
\end{tabular}}

\caption{Подбор по сетке Nb}
\label{grid:nb}
\end{table}



\end{subsubsection}


\begin{subsubsection}{Дерево решений}

В деревьях решений


\begin{table}[H]
\centering
{\begin{tabular}{|l|l|l|c|c|}
\hline
\textbf{Criterion} & \textbf{Max depth} & \textbf{Splitter} & \textbf{Accuracy} & \textbf{Std} \\
\hline
Gini & 1  & best & 0.84177 & 0.00516 \\
\hline
Gini & 1  & random &  0.54702 & 0.03000 \\
\hline
Gini & 2  & best & 0.84177 & 0.00516 \\
\hline
Gini &  2 & random  & 0.59500 & 0.02743 \\
\hline
Gini & 3  & best & 0.87258 & 0.00449 \\
\hline
Gini & 3  & random & 0.64052 & 0.07874 \\
\hline
Gini & None  & best & 0.88475 & 0.00386 \\
\hline
Gini &  None & random  & 0.87360 & 0.00496 \\
\hline
Entropy & 1  & best & 0.84182 & 0.00480 \\
\hline
Entropy & 1  & random & 0.55109 & 0.03190 \\
\hline
Entropy  & 2  & best & 0.84182 & 0.00480 \\
\hline
Entropy  &  2 & random  & 0.58652 & 0.03546 \\
\hline
Entropy  & 3  & best &  0.86879 & 0.00419 \\
\hline
Entropy  & 3  & random & 0.67764 & 0.06590 \\
\hline
Entropy & None  & best & 0.88884 & 0.00441 \\
\hline
Entropy &  None & random  & 0.87409 & 0.00369 \\
\hline
\end{tabular}}

\caption{Подбор по сетке DT}
\label{grid:dt}
\end{table}


\end{subsubsection}


\begin{subsubsection}{Random forest}

  \begin{table}[H]
  \centering
  {\begin{tabular}{|l|l|l|l|c|c|}
  \hline
  \textbf{Estimators} & \textbf{Criterion} & \textbf{Max Depth} & \textbf{Bootstrap} & \textbf{Accuracy} & \textbf{Std} \\
  \hline
  1 & 1  & best & 0.84177 & 0.00516 \\
  \hline
  2 & 1  & random &  0.54702 & 0.03000 \\
  \hline
  Gini & 2  & best & 0.84177 & 0.00516 \\
  \hline
  Gini &  2 & random  & 0.59500 & 0.02743 \\
  \hline
  Gini & 3  & best & 0.87258 & 0.00449 \\
  \hline
  Gini & 3  & random & 0.64052 & 0.07874 \\
  \hline
  Gini & None  & best & 0.88475 & 0.00386 \\
  \hline
  Gini &  None & random  & 0.87360 & 0.00496 \\
  \hline
  Entropy & 1  & best & 0.84182 & 0.00480 \\
  \hline
  Entropy & 1  & random & 0.55109 & 0.03190 \\
  \hline
  Entropy  & 2  & best & 0.84182 & 0.00480 \\
  \hline
  Entropy  &  2 & random  & 0.58652 & 0.03546 \\
  \hline
  Entropy  & 3  & best &  0.86879 & 0.00419 \\
  \hline
  Entropy  & 3  & random & 0.67764 & 0.06590 \\
  \hline
  Entropy & None  & best & 0.88884 & 0.00441 \\
  \hline
  Entropy &  None & random  & 0.87409 & 0.00369 \\
  \hline
    1 & 1  & best & 0.84177 & 0.00516 \\
  \hline
  2 & 1  & random &  0.54702 & 0.03000 \\
  \hline
  Gini & 2  & best & 0.84177 & 0.00516 \\
  \hline
  Gini &  2 & random  & 0.59500 & 0.02743 \\
  \hline
  Gini & 3  & best & 0.87258 & 0.00449 \\
  \hline
  Gini & 3  & random & 0.64052 & 0.07874 \\
  \hline
  Gini & None  & best & 0.88475 & 0.00386 \\
  \hline
  Gini &  None & random  & 0.87360 & 0.00496 \\
  \hline
  Entropy & 1  & best & 0.84182 & 0.00480 \\
  \hline
  Entropy & 1  & random & 0.55109 & 0.03190 \\
  \hline
  Entropy  & 2  & best & 0.84182 & 0.00480 \\
  \hline
  Entropy  &  2 & random  & 0.58652 & 0.03546 \\
  \hline
  Entropy  & 3  & best &  0.86879 & 0.00419 \\
  \hline
  Entropy  & 3  & random & 0.67764 & 0.06590 \\
  \hline
  Entropy & None  & best & 0.88884 & 0.00441 \\
  \hline
  Entropy &  None & random  & 0.87409 & 0.00369 \\
  \hline
  Entropy & 1  & best & 0.84182 & 0.00480 \\
  \hline
  Entropy & 1  & random & 0.55109 & 0.03190 \\
  \hline
  Entropy  & 2  & best & 0.84182 & 0.00480 \\
  \hline
  Entropy  &  2 & random  & 0.58652 & 0.03546 \\
  \hline
  Entropy  & 3  & best &  0.86879 & 0.00419 \\
  \hline
  Entropy  & 3  & random & 0.67764 & 0.06590 \\
  \hline
  Entropy & None  & best & 0.88884 & 0.00441 \\
  \hline
  Entropy &  None & random  & 0.87409 & 0.00369 \\
  \hline
  \end{tabular}}

  \caption{Подбор по сетке DT}
  \label{grid:dt}
  \end{table}



\end{subsubsection}

\end{subsection}

\begin{subsection}{Выбор классификатора}

Результаты алгорита kNN


В качестве метрик качества классификаторов были выбраны полнота (R), точность (P) и F-1 статистика. Метрика полноты представляет из себя отношение числа сообщений, корректно классифицированных как спам (True Positives) и общего числа спамовых сообщений (True Positives + False Negatives):

\begin{equation}
  R = \frac{True Positives}{True Positives + False Negatives}
\end{equation}

Точность - это отношение числа сообщений, корректно классифицированных как спам (True Positives) и общего числа  сообщений, классифицированных как спам (True Positives + False Positives):

\begin{equation}
  P = \frac{True Positives}{True Positives + False Positives}
\end{equation}

F-1 статистика может быть проинтерпретирована как гармоническое среднее между метриками точности и полноты:

\begin{equation}
  F1 = \frac{2 \times P \times R}{P + R}
\end{equation}



Для обучения из имеющегося набора данных случайным образом был использован







\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Классификатор} & \textbf{Точность} & \textbf{Полнота} & \textbf{F-1}  \\
\hline
kNN & 86\% & 86\% & 86\% \\
\hline
Решающее дерево & 89\% & 89\% & 89\% \\
\hline
Наивный байесовский классификатор & 76\% & 76\% & 76\% \\
\hline
\textbf{Случайные леса} & \textbf{93\%} & \textbf{93\%} & \textbf{93\%} \\
\hline
SVM & 86\% & 86\% & 86\%  \\
\hline
\end{tabular}}

\caption{Сравнение показателей классификатора}
\label{tab:results}
\end{table}




\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|l|c|}
\hline
\textbf{Группа признаков} & \textbf{Время на вычисление (сек.) для 1000 твитов} \\
\hline
Пользовательские признаки & 0.0057 \\
\hline
Признаки контента & 1.0448 \\
\hline
\end{tabular}}

\caption{Время вычисления признаков}
\label{tab:comptime}
\end{table}



\end{subsection}


Для обучения было использовано 40\% выборки. В процессе оптимизации гиперпараметров для каждого классификатора использовалась кросс-валидация (CV-10). Кросс-валидация способствует минимизации риска  переобучения, и следовательно, последующего смещения в оценке качества классификатора. Оптимизация проводилась для обоих групп признаков (каждый признак был нормальзован и принимал значение от 0 до \[-1, 1\]). Кроме этого для настройки SVM было отобрано 30\% наиболее значимых признаков по критерию Хи квадрат, поскольку это более оптимально с точки зрения вычислительной нагрузки. После этого  каждый из алгоримов оценивался на оставшихся 60\% данных с использованием обоих групп признаков и CV-10. Также как и при поиске по сетке, каждый признак был нормализован  и для SVM использовалось 30\% наиболее значимых признаков по критерию Хи квадрат.

Как видно в таблице \ref{tab:results}, классификаторы на основе дерева показали лучший результат. Random Forests обошел остальные классификаторы по показателю F-1. Наилучший известный мне результат в данной задаче был достигнут у Lee \cite{Lee} и составляет 98\% по той же оценке F-1. Однако Lee использовал историчные признаки, что является ограничением в нашем случае, следовательно, результат можно считать вполне неплохим. Это говорит о том, что лишь те признаки, которые можно извлечь непосредственно из твита и его метаданных,  могут служить хорошей основой для классификатора

\begin{subsection}{Оценка признаков}
Для оценки значимости признаков были  использованы различные комбинации  при обучениии классификаторов. Результат очевидно продемонстрировал, что наибольший вес с точки зрения F-1 оценки имеют признаки из пользовательской группы. Комбинации с использованием признаков из этой группы с различными признаками из группы признаков контента дают лишь небольшой прирост в F-1 показателе. 

Еще одним важным аспектом который желательно учитывать при отборе признаков это время их вычисления, особенно если существует необходимость применения классификатора в режиме реального времени. Таблица \ref{tab:comptime} демонстрирует время вычисления каждой из групп признаков для 1000 твитов со следующими параметрами машины:
\begin{enumerate}
\item CPU: Intel Core i7
\item RAM: 16GB
\item OS: Ubuntu 16.04 64-bit
\end{enumerate}

Следует отметить, что обе группы признаков вычисляются за приемлимое время, что отвечает условиям поставленной задачи.
\end{subsection}


\begin{subsection}{Анализ результатов}

\end{subsection}









\end{section}
