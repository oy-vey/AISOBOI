{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# settings\n",
    "LEARNING_RATE = 1e-4\n",
    "# set to 20000 on local environment to get 0.99 accuracy\n",
    "TRAINING_ITERATIONS = 20000        \n",
    "    \n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# set to 0 to train on all available data\n",
    "VALIDATION_SIZE = 2000\n",
    "\n",
    "# image number to output\n",
    "IMAGE_TO_DISPLAY = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data(42000,785)\n",
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read training data from CSV file \n",
    "data = pd.read_csv('./train.csv')\n",
    "\n",
    "print('data({0[0]},{0[1]})'.format(data.shape))\n",
    "print (data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images(42000,784)\n"
     ]
    }
   ],
   "source": [
    "images = data.iloc[:,1:].values\n",
    "images = images.astype(np.float)\n",
    "\n",
    "# convert from [0:255] => [0.0:1.0]\n",
    "images = np.multiply(images, 1.0 / 255.0)\n",
    "\n",
    "print('images({0[0]},{0[1]})'.format(images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_size => 784\n",
      "image_width => 28\n",
      "image_height => 28\n"
     ]
    }
   ],
   "source": [
    "image_size = images.shape[1]\n",
    "print ('image_size => {0}'.format(image_size))\n",
    "\n",
    "# in this case all images are square\n",
    "image_width = image_height = np.ceil(np.sqrt(image_size)).astype(np.uint8)\n",
    "\n",
    "print ('image_width => {0}\\nimage_height => {1}'.format(image_width,image_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e50cd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display image\n",
    "def display(img):\n",
    "    \n",
    "    # (784) => (28,28)\n",
    "    one_image = img.reshape(image_width,image_height)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.imshow(one_image, cmap=cm.binary)\n",
    "\n",
    "# output image     \n",
    "display(images[IMAGE_TO_DISPLAY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_flat(42000)\n",
      "labels_flat[10] => 8\n"
     ]
    }
   ],
   "source": [
    "labels_flat = data[[0]].values.ravel()\n",
    "\n",
    "print('labels_flat({0})'.format(len(labels_flat)))\n",
    "print ('labels_flat[{0}] => {1}'.format(IMAGE_TO_DISPLAY,labels_flat[IMAGE_TO_DISPLAY]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_count => 10\n"
     ]
    }
   ],
   "source": [
    "labels_count = np.unique(labels_flat).shape[0]\n",
    "\n",
    "print('labels_count => {0}'.format(labels_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels(42000,10)\n",
      "labels[10] => [0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# convert class labels from scalars to one-hot vectors\n",
    "# 0 => [1 0 0 0 0 0 0 0 0 0]\n",
    "# 1 => [0 1 0 0 0 0 0 0 0 0]\n",
    "# ...\n",
    "# 9 => [0 0 0 0 0 0 0 0 0 1]\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "labels = dense_to_one_hot(labels_flat, labels_count)\n",
    "labels = labels.astype(np.uint8)\n",
    "\n",
    "print('labels({0[0]},{0[1]})'.format(labels.shape))\n",
    "print ('labels[{0}] => {1}'.format(IMAGE_TO_DISPLAY,labels[IMAGE_TO_DISPLAY]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images(40000,784)\n",
      "validation_images(2000,784)\n"
     ]
    }
   ],
   "source": [
    "# split data into training & validation\n",
    "validation_images = images[:VALIDATION_SIZE]\n",
    "validation_labels = labels[:VALIDATION_SIZE]\n",
    "\n",
    "train_images = images[VALIDATION_SIZE:]\n",
    "train_labels = labels[VALIDATION_SIZE:]\n",
    "\n",
    "\n",
    "print('train_images({0[0]},{0[1]})'.format(train_images.shape))\n",
    "print('validation_images({0[0]},{0[1]})'.format(validation_images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pooling\n",
    "# [[0,3],\n",
    "#  [4,2]] => 4\n",
    "\n",
    "# [[0,1],\n",
    "#  [1,1]] => 1\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input & output of NN\n",
    "\n",
    "# images\n",
    "x = tf.placeholder('float', shape=[None, image_size])\n",
    "# labels\n",
    "y_ = tf.placeholder('float', shape=[None, labels_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first convolutional layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# (40000,784) => (40000,28,28,1)\n",
    "image = tf.reshape(x, [-1,image_width , image_height,1])\n",
    "#print (image.get_shape()) # =>(40000,28,28,1)\n",
    "\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1)\n",
    "#print (h_conv1.get_shape()) # => (40000, 28, 28, 32)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "#print (h_pool1.get_shape()) # => (40000, 14, 14, 32)\n",
    "\n",
    "\n",
    "# Prepare for visualization\n",
    "# display 32 fetures in 4 by 8 grid\n",
    "layer1 = tf.reshape(h_conv1, (-1, image_height, image_width, 4 ,8))  \n",
    "\n",
    "# reorder so the channels are in the first dimension, x and y follow.\n",
    "layer1 = tf.transpose(layer1, (0, 3, 1, 4,2))\n",
    "\n",
    "layer1 = tf.reshape(layer1, (-1, image_height*4, image_width*8)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# second convolutional layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "#print (h_conv2.get_shape()) # => (40000, 14,14, 64)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "#print (h_pool2.get_shape()) # => (40000, 7, 7, 64)\n",
    "\n",
    "# Prepare for visualization\n",
    "# display 64 fetures in 4 by 16 grid\n",
    "layer2 = tf.reshape(h_conv2, (-1, 14, 14, 4 ,16))  \n",
    "\n",
    "# reorder so the channels are in the first dimension, x and y follow.\n",
    "layer2 = tf.transpose(layer2, (0, 3, 1, 4,2))\n",
    "\n",
    "layer2 = tf.reshape(layer2, (-1, 14*4, 14*16)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# densely connected layer\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "# (40000, 7, 7, 64) => (40000, 3136)\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "#print (h_fc1.get_shape()) # => (40000, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropout\n",
    "keep_prob = tf.placeholder('float')\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# readout layer for deep net\n",
    "W_fc2 = weight_variable([1024, labels_count])\n",
    "b_fc2 = bias_variable([labels_count])\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "#print (y.get_shape()) # => (40000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost function\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "\n",
    "# optimisation function\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "# evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction function\n",
    "#[0.1, 0.9, 0.2, 0.1, 0.1 0.3, 0.5, 0.1, 0.2, 0.3] => 1\n",
    "predict = tf.argmax(y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = train_images.shape[0]\n",
    "\n",
    "# serve data by batches\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    global train_images\n",
    "    global train_labels\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # when all trainig data have been already used, it is reorder randomly    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        train_images = train_images[perm]\n",
    "        train_labels = train_labels[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return train_images[start:end], train_labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-48-b1d8122bed2f>:2: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "# start TensorFlow session\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy / validation_accuracy => 0.08 / 0.12 for step 0\n",
      "training_accuracy / validation_accuracy => 0.06 / 0.08 for step 1\n",
      "training_accuracy / validation_accuracy => 0.04 / 0.08 for step 2\n",
      "training_accuracy / validation_accuracy => 0.10 / 0.08 for step 3\n",
      "training_accuracy / validation_accuracy => 0.06 / 0.12 for step 4\n",
      "training_accuracy / validation_accuracy => 0.10 / 0.16 for step 5\n",
      "training_accuracy / validation_accuracy => 0.16 / 0.22 for step 6\n",
      "training_accuracy / validation_accuracy => 0.28 / 0.26 for step 7\n",
      "training_accuracy / validation_accuracy => 0.08 / 0.28 for step 8\n",
      "training_accuracy / validation_accuracy => 0.24 / 0.28 for step 9\n",
      "training_accuracy / validation_accuracy => 0.22 / 0.28 for step 10\n",
      "training_accuracy / validation_accuracy => 0.36 / 0.52 for step 20\n",
      "training_accuracy / validation_accuracy => 0.48 / 0.62 for step 30\n",
      "training_accuracy / validation_accuracy => 0.72 / 0.70 for step 40\n",
      "training_accuracy / validation_accuracy => 0.66 / 0.80 for step 50\n",
      "training_accuracy / validation_accuracy => 0.78 / 0.84 for step 60\n",
      "training_accuracy / validation_accuracy => 0.88 / 0.82 for step 70\n",
      "training_accuracy / validation_accuracy => 0.86 / 0.86 for step 80\n",
      "training_accuracy / validation_accuracy => 0.80 / 0.92 for step 90\n",
      "training_accuracy / validation_accuracy => 0.90 / 0.90 for step 100\n",
      "training_accuracy / validation_accuracy => 0.96 / 0.92 for step 200\n",
      "training_accuracy / validation_accuracy => 0.96 / 0.96 for step 300\n",
      "training_accuracy / validation_accuracy => 0.90 / 0.96 for step 400\n",
      "training_accuracy / validation_accuracy => 1.00 / 0.96 for step 500\n",
      "training_accuracy / validation_accuracy => 0.92 / 0.98 for step 600\n",
      "training_accuracy / validation_accuracy => 0.90 / 0.96 for step 700\n",
      "training_accuracy / validation_accuracy => 1.00 / 0.96 for step 800\n",
      "training_accuracy / validation_accuracy => 0.96 / 0.96 for step 900\n",
      "training_accuracy / validation_accuracy => 0.98 / 0.96 for step 1000\n",
      "training_accuracy / validation_accuracy => 0.98 / 0.96 for step 2000\n",
      "training_accuracy / validation_accuracy => 1.00 / 0.98 for step 3000\n",
      "training_accuracy / validation_accuracy => 0.98 / 1.00 for step 4000\n",
      "training_accuracy / validation_accuracy => 1.00 / 0.98 for step 5000\n",
      "training_accuracy / validation_accuracy => 0.98 / 1.00 for step 6000\n",
      "training_accuracy / validation_accuracy => 0.98 / 1.00 for step 7000\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 8000\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 9000\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 10000\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 19999\n"
     ]
    }
   ],
   "source": [
    "# visualisation variables\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "x_range = []\n",
    "\n",
    "display_step=1\n",
    "\n",
    "for i in range(TRAINING_ITERATIONS):\n",
    "\n",
    "    #get new batch\n",
    "    batch_xs, batch_ys = next_batch(BATCH_SIZE)        \n",
    "\n",
    "    # check progress on every 1st,2nd,...,10th,20th,...,100th... step\n",
    "    if i%display_step == 0 or (i+1) == TRAINING_ITERATIONS:\n",
    "        \n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_xs, \n",
    "                                                  y_: batch_ys, \n",
    "                                                  keep_prob: 1.0})       \n",
    "        if(VALIDATION_SIZE):\n",
    "            validation_accuracy = accuracy.eval(feed_dict={ x: validation_images[0:BATCH_SIZE], \n",
    "                                                            y_: validation_labels[0:BATCH_SIZE], \n",
    "                                                            keep_prob: 1.0})                                  \n",
    "            print('training_accuracy / validation_accuracy => %.2f / %.2f for step %d'%(train_accuracy, validation_accuracy, i))\n",
    "            \n",
    "            validation_accuracies.append(validation_accuracy)\n",
    "            \n",
    "        else:\n",
    "             print('training_accuracy => %.4f for step %d'%(train_accuracy, i))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        x_range.append(i)\n",
    "        \n",
    "        # increase display_step\n",
    "        if i%(display_step*10) == 0 and i:\n",
    "            display_step *= 10\n",
    "    # train on batch\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: DROPOUT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_accuracy => 0.9935\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhoiIRKKEISIikShhiIhIJEoYIiISyf8HpYSwLNrBSV4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e6b6a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# check final accuracy on validation set  \n",
    "if(VALIDATION_SIZE):\n",
    "    validation_accuracy = accuracy.eval(feed_dict={x: validation_images, \n",
    "                                                   y_: validation_labels, \n",
    "                                                   keep_prob: 1.0})\n",
    "    print('validation_accuracy => %.4f'%validation_accuracy)\n",
    "    plt.plot(x_range, train_accuracies,'-b', label='Training')\n",
    "    plt.plot(x_range, validation_accuracies,'-g', label='Validation')\n",
    "    plt.legend(loc='lower right', frameon=False)\n",
    "    plt.ylim(ymax = 1.1, ymin = 0.7)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_images(28000,784)\n",
      "predicted_lables(28000)\n",
      "predicted_lables[10] => 5.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e513748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read test data from CSV file \n",
    "test_images = pd.read_csv('./test.csv').values\n",
    "test_images = test_images.astype(np.float)\n",
    "\n",
    "# convert from [0:255] => [0.0:1.0]\n",
    "test_images = np.multiply(test_images, 1.0 / 255.0)\n",
    "\n",
    "print('test_images({0[0]},{0[1]})'.format(test_images.shape))\n",
    "\n",
    "\n",
    "# predict test set\n",
    "#predicted_lables = predict.eval(feed_dict={x: test_images, keep_prob: 1.0})\n",
    "\n",
    "# using batches is more resource efficient\n",
    "predicted_lables = np.zeros(test_images.shape[0])\n",
    "for i in range(0,test_images.shape[0]//BATCH_SIZE):\n",
    "    predicted_lables[i*BATCH_SIZE : (i+1)*BATCH_SIZE] = predict.eval(feed_dict={x: test_images[i*BATCH_SIZE : (i+1)*BATCH_SIZE], \n",
    "                                                                                keep_prob: 1.0})\n",
    "\n",
    "\n",
    "print('predicted_lables({0})'.format(len(predicted_lables)))\n",
    "\n",
    "# output test image and prediction\n",
    "display(test_images[IMAGE_TO_DISPLAY])\n",
    "print ('predicted_lables[{0}] => {1}'.format(IMAGE_TO_DISPLAY,predicted_lables[IMAGE_TO_DISPLAY]))\n",
    "\n",
    "# save results\n",
    "np.savetxt('submission.csv', \n",
    "           np.c_[range(1,len(test_images)+1),predicted_lables], \n",
    "           delimiter=',', \n",
    "           header = 'ImageId,Label', \n",
    "           comments = '', \n",
    "           fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
